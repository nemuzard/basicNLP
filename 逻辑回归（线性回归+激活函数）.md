# 逻辑回归（线性回归+激活函数）

## 5.1 激活函数

- 给线性方程加上一些非线性因素 - 让原本直线的方程学会拐弯

<aside>
👌 整体向上或者向下才可以使用线性回归

</aside>

- 比如股票预测，股票上上下下，单纯的线性回归没办法满足
- 激活函数的值一般都在很小的区间，加上激活函数之后，线性回归的值也会在一个非常小的区间

<aside>
🔑 激活函数让线性回归输出的值不那么大，也不会有梯度爆炸了

</aside>

## 5.2 回归&分类

1. 分类：选择题：根据已有的选择
    1. 猫狗识别
    2. 真假新闻
2. 回归：应用题：根据以往的情况给出一个准确的“数字”
    1. 复杂
    2. 目标检测 - 回归+分类

## 5.3 逻辑回归的实现

关于reshape的使用：

```python
'''
reshape, NumPy方法，用来改变数组的形状。x个参数就是x-d数组
'''
# 2d，-1表示自动计算row num，1表示每一行1个数据
.reshape(-1,1)
# 会创建一个3d数组，维度为a,b,c 
.reshape(a, b, c)
```

- 逻辑回归 = 线性回归 + 激活
- $X @ K = Pre$  ↔ Label
- 在我们的猫狗分类例子里维度：X(14,2), K-特征-(2,1), pre(14,1)
- 不过pre会得到任何实数值，但是我们的标签是0和1，我们需要让预测值也这样，所以需要特殊处理 → 激活函数的作用（这种情况只起到一个限制区域的作用）
- 激活函数: pre → sigmoid → label
    - $S(x) = 1/exp(-x)$
    - pre 在0,1区间就可以和label计算loss

## 5.4 数学推导

$P = X @ K, pre = sig(P)= 1/(1+exp(-p))$ Note：K 一开始是随机的，可以根据x和pre的维度推出K的维度

（m*n）and (n*b) → (m*b)

我们的例子 (14,2)*___ = (14.1) → (2,1)

$Loss = label * log(pre) + (1-klabel) * log(1-pre)$

<aside>
👻 这个569有学过叫做binary cross-entropy loss, which is used for binary classification

</aside>

- $loss = -[ylog(\hat p) + (1-y)log(1-\hat p)]$
- Where $\hat p$  is the predicted probability of the positive class (pre)
- y is the true label
- $G = \partial loss / \partial P = (\partial loss/\partial pre)*(\partial pre/\partial P)$
- $G = pre - label$

<aside>
😾 只要是softmax或者sigmoid + cross-entropy loss， G都是这个数值，前面可能有某个Constant

</aside>

- $\partial loss/\partial k = X^T @G$
- update K: $k = k - lr *(\partial loss/\partial k)$

## 5.5 forward

```python
'''
给所有的数据（猫和狗）垂直拼接，从两个7*2 变成一个 14*2。 V是vertical，所以是垂直
'''
X = np.vstack((dogs,cats))

'''
随机K, 满足均值0,方差1的一个平均数,大小2个
'''
k = np.random.normal(0,1,size=(2,1))

'''
在每个epoch里面进行计算
'''

p = X @ k +b #计算结果
#加上激活函数 sigmoid, 预测
pre = sigmoid(p)
# pre 和 labels 都是矩阵， 所以最后的loss要算sum，因为要给所有的都加上
# 在pytorch里面，下面的loss要算个平均，因为要是10000数组的话有点吓人
loss = np.sum(labels * np.log(pre) + (1-labels)*np.log(1-pre))
loss = np.mean(loss)
loss = -loss # 给loss变成正数，为了便于计算
# delta_b = G, 但是G是数字。 B只是一个偏置项，loss对p的导数和loss 对b的导数是一样的
delta_b = np.sum(G)
# update
k = k - lr * delta_k
b = b - lr * delta_b
```

<aside>
🔑 详细解释

[Δ*b*=*G*](https://www.notion.so/b-G-fcad4152ba3141a487da4924d6b9c57b?pvs=21)

</aside>

预测

```python
f1 = float(input("毛发长："))
f2 = float(input("脚长："))
# 1 row , 2 col
test_x = np.array([f1,f2]).reshape(1,2)
p = test_x @ k + b
p = sigmoid(p)
if p>0.5:
    print("cat")
else:
    print("dog")
```

<aside>
🔥 单纯使用 1/(1+exp(-x)) 会出现RuntimeWarning: overflow encountered in exp return 1 / (1 + np.exp(-x))

</aside>

原因

- 这个 RuntimeWarning: overflow encountered in exp 警告是在使用 np.exp(-x) 函数时常见的，当 -x 的值非常大（即 x 是一个很大的负数）时，exp(-x) 的计算结果会尝试返回一个非常大的数，超出了 Python 可以表示的浮点数的范围

解决办法：

1. 数值稳定的 Sigmoid 实现： 你可以使用一种更稳定的方式来计算 Sigmoid 函数，以避免在计算 exp(-x)时发生数值溢出。

```python
import numpy as np

def sigmoid(x):
    """Compute the sigmoid function in a numerically stable way."""
    # For positive values of x
    pos_mask = (x >= 0)
    # For negative values of x, shift the computation to avoid overflow
    neg_mask = (x < 0)
    z = np.zeros_like(x, dtype=float)
    z[pos_mask] = np.exp(-x[pos_mask])
    z[neg_mask] = np.exp(x[neg_mask])
    top = np.ones_like(x, dtype=float)
    top[neg_mask] = z[neg_mask]
    return top / (1 + z)
```

1. 使用库函数： 很多深度学习库如 TensorFlow 或 PyTorch 都内置了数值稳定的 Sigmoid 函数。如果你正在使用这些库，可以直接调用库函数来避免这类问题