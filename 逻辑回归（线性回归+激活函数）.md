# é€»è¾‘å›å½’ï¼ˆçº¿æ€§å›å½’+æ¿€æ´»å‡½æ•°ï¼‰

## 5.1 æ¿€æ´»å‡½æ•°

- ç»™çº¿æ€§æ–¹ç¨‹åŠ ä¸Šä¸€äº›éçº¿æ€§å› ç´  - è®©åŸæœ¬ç›´çº¿çš„æ–¹ç¨‹å­¦ä¼šæ‹å¼¯

<aside>
ğŸ‘Œ æ•´ä½“å‘ä¸Šæˆ–è€…å‘ä¸‹æ‰å¯ä»¥ä½¿ç”¨çº¿æ€§å›å½’

</aside>

- æ¯”å¦‚è‚¡ç¥¨é¢„æµ‹ï¼Œè‚¡ç¥¨ä¸Šä¸Šä¸‹ä¸‹ï¼Œå•çº¯çš„çº¿æ€§å›å½’æ²¡åŠæ³•æ»¡è¶³
- æ¿€æ´»å‡½æ•°çš„å€¼ä¸€èˆ¬éƒ½åœ¨å¾ˆå°çš„åŒºé—´ï¼ŒåŠ ä¸Šæ¿€æ´»å‡½æ•°ä¹‹åï¼Œçº¿æ€§å›å½’çš„å€¼ä¹Ÿä¼šåœ¨ä¸€ä¸ªéå¸¸å°çš„åŒºé—´

<aside>
ğŸ”‘ æ¿€æ´»å‡½æ•°è®©çº¿æ€§å›å½’è¾“å‡ºçš„å€¼ä¸é‚£ä¹ˆå¤§ï¼Œä¹Ÿä¸ä¼šæœ‰æ¢¯åº¦çˆ†ç‚¸äº†

</aside>

## 5.2 å›å½’&åˆ†ç±»

1. åˆ†ç±»ï¼šé€‰æ‹©é¢˜ï¼šæ ¹æ®å·²æœ‰çš„é€‰æ‹©
    1. çŒ«ç‹—è¯†åˆ«
    2. çœŸå‡æ–°é—»
2. å›å½’ï¼šåº”ç”¨é¢˜ï¼šæ ¹æ®ä»¥å¾€çš„æƒ…å†µç»™å‡ºä¸€ä¸ªå‡†ç¡®çš„â€œæ•°å­—â€
    1. å¤æ‚
    2. ç›®æ ‡æ£€æµ‹ - å›å½’+åˆ†ç±»

## 5.3 é€»è¾‘å›å½’çš„å®ç°

å…³äºreshapeçš„ä½¿ç”¨ï¼š

```python
'''
reshape, NumPyæ–¹æ³•ï¼Œç”¨æ¥æ”¹å˜æ•°ç»„çš„å½¢çŠ¶ã€‚xä¸ªå‚æ•°å°±æ˜¯x-dæ•°ç»„
'''
# 2dï¼Œ-1è¡¨ç¤ºè‡ªåŠ¨è®¡ç®—row numï¼Œ1è¡¨ç¤ºæ¯ä¸€è¡Œ1ä¸ªæ•°æ®
.reshape(-1,1)
# ä¼šåˆ›å»ºä¸€ä¸ª3dæ•°ç»„ï¼Œç»´åº¦ä¸ºa,b,c 
.reshape(a, b, c)
```

- é€»è¾‘å›å½’ = çº¿æ€§å›å½’ + æ¿€æ´»
- $X @ K = Pre$  â†” Label
- åœ¨æˆ‘ä»¬çš„çŒ«ç‹—åˆ†ç±»ä¾‹å­é‡Œç»´åº¦ï¼šX(14,2), K-ç‰¹å¾-(2,1), pre(14,1)
- ä¸è¿‡preä¼šå¾—åˆ°ä»»ä½•å®æ•°å€¼ï¼Œä½†æ˜¯æˆ‘ä»¬çš„æ ‡ç­¾æ˜¯0å’Œ1ï¼Œæˆ‘ä»¬éœ€è¦è®©é¢„æµ‹å€¼ä¹Ÿè¿™æ ·ï¼Œæ‰€ä»¥éœ€è¦ç‰¹æ®Šå¤„ç† â†’ æ¿€æ´»å‡½æ•°çš„ä½œç”¨ï¼ˆè¿™ç§æƒ…å†µåªèµ·åˆ°ä¸€ä¸ªé™åˆ¶åŒºåŸŸçš„ä½œç”¨ï¼‰
- æ¿€æ´»å‡½æ•°: pre â†’ sigmoid â†’ label
    - $S(x) = 1/exp(-x)$
    - pre åœ¨0,1åŒºé—´å°±å¯ä»¥å’Œlabelè®¡ç®—loss

## 5.4 æ•°å­¦æ¨å¯¼

$P = X @ K, pre = sig(P)= 1/(1+exp(-p))$ Noteï¼šK ä¸€å¼€å§‹æ˜¯éšæœºçš„ï¼Œå¯ä»¥æ ¹æ®xå’Œpreçš„ç»´åº¦æ¨å‡ºKçš„ç»´åº¦

ï¼ˆm*nï¼‰and (n*b) â†’ (m*b)

æˆ‘ä»¬çš„ä¾‹å­ (14,2)*___ = (14.1) â†’ (2,1)

$Loss = label * log(pre) + (1-klabel) * log(1-pre)$

<aside>
ğŸ‘» è¿™ä¸ª569æœ‰å­¦è¿‡å«åšbinary cross-entropy loss, which is used for binary classification

</aside>

- $loss = -[ylog(\hat p) + (1-y)log(1-\hat p)]$
- Where $\hat p$  is the predicted probability of the positive class (pre)
- y is the true label
- $G = \partial loss / \partial P = (\partial loss/\partial pre)*(\partial pre/\partial P)$
- $G = pre - label$

<aside>
ğŸ˜¾ åªè¦æ˜¯softmaxæˆ–è€…sigmoid + cross-entropy lossï¼Œ Géƒ½æ˜¯è¿™ä¸ªæ•°å€¼ï¼Œå‰é¢å¯èƒ½æœ‰æŸä¸ªConstant

</aside>

- $\partial loss/\partial k = X^T @G$
- update K: $k = k - lr *(\partial loss/\partial k)$

## 5.5 forward

```python
'''
ç»™æ‰€æœ‰çš„æ•°æ®ï¼ˆçŒ«å’Œç‹—ï¼‰å‚ç›´æ‹¼æ¥ï¼Œä»ä¸¤ä¸ª7*2 å˜æˆä¸€ä¸ª 14*2ã€‚ Væ˜¯verticalï¼Œæ‰€ä»¥æ˜¯å‚ç›´
'''
X = np.vstack((dogs,cats))

'''
éšæœºK, æ»¡è¶³å‡å€¼0,æ–¹å·®1çš„ä¸€ä¸ªå¹³å‡æ•°,å¤§å°2ä¸ª
'''
k = np.random.normal(0,1,size=(2,1))

'''
åœ¨æ¯ä¸ªepoché‡Œé¢è¿›è¡Œè®¡ç®—
'''

p = X @ k +b #è®¡ç®—ç»“æœ
#åŠ ä¸Šæ¿€æ´»å‡½æ•° sigmoid, é¢„æµ‹
pre = sigmoid(p)
# pre å’Œ labels éƒ½æ˜¯çŸ©é˜µï¼Œ æ‰€ä»¥æœ€åçš„lossè¦ç®—sumï¼Œå› ä¸ºè¦ç»™æ‰€æœ‰çš„éƒ½åŠ ä¸Š
# åœ¨pytorché‡Œé¢ï¼Œä¸‹é¢çš„lossè¦ç®—ä¸ªå¹³å‡ï¼Œå› ä¸ºè¦æ˜¯10000æ•°ç»„çš„è¯æœ‰ç‚¹å“äºº
loss = np.sum(labels * np.log(pre) + (1-labels)*np.log(1-pre))
loss = np.mean(loss)
loss = -loss # ç»™losså˜æˆæ­£æ•°ï¼Œä¸ºäº†ä¾¿äºè®¡ç®—
# delta_b = G, ä½†æ˜¯Gæ˜¯æ•°å­—ã€‚ Båªæ˜¯ä¸€ä¸ªåç½®é¡¹ï¼Œlosså¯¹pçš„å¯¼æ•°å’Œloss å¯¹bçš„å¯¼æ•°æ˜¯ä¸€æ ·çš„
delta_b = np.sum(G)
# update
k = k - lr * delta_k
b = b - lr * delta_b
```

<aside>
ğŸ”‘ è¯¦ç»†è§£é‡Š

[Î”*b*=*G*](https://www.notion.so/b-G-fcad4152ba3141a487da4924d6b9c57b?pvs=21)

</aside>

é¢„æµ‹

```python
f1 = float(input("æ¯›å‘é•¿ï¼š"))
f2 = float(input("è„šé•¿ï¼š"))
# 1 row , 2 col
test_x = np.array([f1,f2]).reshape(1,2)
p = test_x @ k + b
p = sigmoid(p)
if p>0.5:
    print("cat")
else:
    print("dog")
```

<aside>
ğŸ”¥ å•çº¯ä½¿ç”¨ 1/(1+exp(-x)) ä¼šå‡ºç°RuntimeWarning: overflow encountered in exp return 1 / (1 + np.exp(-x))

</aside>

åŸå› 

- è¿™ä¸ª RuntimeWarning: overflow encountered in exp è­¦å‘Šæ˜¯åœ¨ä½¿ç”¨ np.exp(-x) å‡½æ•°æ—¶å¸¸è§çš„ï¼Œå½“ -x çš„å€¼éå¸¸å¤§ï¼ˆå³ x æ˜¯ä¸€ä¸ªå¾ˆå¤§çš„è´Ÿæ•°ï¼‰æ—¶ï¼Œexp(-x) çš„è®¡ç®—ç»“æœä¼šå°è¯•è¿”å›ä¸€ä¸ªéå¸¸å¤§çš„æ•°ï¼Œè¶…å‡ºäº† Python å¯ä»¥è¡¨ç¤ºçš„æµ®ç‚¹æ•°çš„èŒƒå›´

è§£å†³åŠæ³•ï¼š

1. æ•°å€¼ç¨³å®šçš„ Sigmoid å®ç°ï¼š ä½ å¯ä»¥ä½¿ç”¨ä¸€ç§æ›´ç¨³å®šçš„æ–¹å¼æ¥è®¡ç®— Sigmoid å‡½æ•°ï¼Œä»¥é¿å…åœ¨è®¡ç®— exp(-x)æ—¶å‘ç”Ÿæ•°å€¼æº¢å‡ºã€‚

```python
import numpy as np

def sigmoid(x):
    """Compute the sigmoid function in a numerically stable way."""
    # For positive values of x
    pos_mask = (x >= 0)
    # For negative values of x, shift the computation to avoid overflow
    neg_mask = (x < 0)
    z = np.zeros_like(x, dtype=float)
    z[pos_mask] = np.exp(-x[pos_mask])
    z[neg_mask] = np.exp(x[neg_mask])
    top = np.ones_like(x, dtype=float)
    top[neg_mask] = z[neg_mask]
    return top / (1 + z)
```

1. ä½¿ç”¨åº“å‡½æ•°ï¼š å¾ˆå¤šæ·±åº¦å­¦ä¹ åº“å¦‚ TensorFlow æˆ– PyTorch éƒ½å†…ç½®äº†æ•°å€¼ç¨³å®šçš„ Sigmoid å‡½æ•°ã€‚å¦‚æœä½ æ­£åœ¨ä½¿ç”¨è¿™äº›åº“ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨åº“å‡½æ•°æ¥é¿å…è¿™ç±»é—®é¢˜